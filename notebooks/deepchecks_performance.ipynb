{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78c6835",
   "metadata": {},
   "source": [
    "# ðŸ† DEEPCHECKS CRIME LA - NIVEAU 3 : PERFORMANCE DU MODÃˆLE\n",
    "\n",
    "## ðŸŽ¯ Objectif\n",
    "Ã‰valuer les performances des modÃ¨les de prÃ©diction de crimes avec Deepchecks :\n",
    "- Analyse des performances par classe de crime\n",
    "- DÃ©tection de biais dans les prÃ©dictions\n",
    "- VÃ©rification de la calibration du modÃ¨le\n",
    "- Analyse des erreurs et cas difficiles\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e124e30",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b700755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ajouter src au path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "print(\"âœ… Imports de base effectuÃ©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fa42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepchecks pour Ã©valuation modÃ¨le\n",
    "try:\n",
    "    from deepchecks.tabular import Dataset\n",
    "    from deepchecks.tabular.suites import model_evaluation\n",
    "    from deepchecks.tabular.checks import (\n",
    "        ModelErrorAnalysis, ConfusionMatrixReport,\n",
    "        RocReport, CalibrationScore,\n",
    "        SegmentPerformance, PredictionDrift,\n",
    "        SimpleModelComparison, WeakSegmentsPerformance\n",
    "    )\n",
    "    DEEPCHECKS_AVAILABLE = True\n",
    "    print(\"âœ… Deepchecks importÃ©\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Deepchecks non disponible: {e}\")\n",
    "    DEEPCHECKS_AVAILABLE = False\n",
    "\n",
    "# ML et mÃ©triques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe92e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "REPORTS_DIR = BASE_DIR / 'reports' / 'deepchecks'\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ† DEEPCHECKS CRIME LA - NIVEAU 3 : PERFORMANCE DU MODÃˆLE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ðŸ“ Base: {BASE_DIR}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a36270",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“¥ Chargement des DonnÃ©es et du ModÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_crime_data():\n",
    "    \"\"\"Charge et prÃ©pare les donnÃ©es pour Ã©valuation\"\"\"\n",
    "    print(\"ðŸ“¦ Chargement des donnÃ©es pour Ã©valuation\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Chercher donnÃ©es processed\n",
    "    processed_files = list((DATA_DIR / 'processed').glob('*.csv'))\n",
    "    raw_files = list((DATA_DIR / 'raw').glob('*.csv'))\n",
    "    \n",
    "    if processed_files:\n",
    "        data_file = processed_files[0]\n",
    "        df = pd.read_csv(data_file)\n",
    "        print(f\"âœ… DonnÃ©es processed: {data_file.name}\")\n",
    "    elif raw_files:\n",
    "        data_file = raw_files[0]\n",
    "        df = pd.read_csv(data_file)\n",
    "        \n",
    "        # Appliquer preprocessing si nÃ©cessaire\n",
    "        try:\n",
    "            from src.data.preprocessing import clean_data\n",
    "            df = clean_data(df)\n",
    "            print(f\"âœ… DonnÃ©es raw + preprocessing: {data_file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Preprocessing Ã©chouÃ©: {e}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Aucun fichier de donnÃ©es trouvÃ©\")\n",
    "    \n",
    "    # PrÃ©parer pour ML (mÃªme logique que train.py)\n",
    "    if 'Crime_Group' not in df.columns and 'Crm Cd Desc' in df.columns:\n",
    "        try:\n",
    "            from src.models.train import map_crime_group_4\n",
    "            df['Crime_Group'] = df['Crm Cd Desc'].apply(map_crime_group_4)\n",
    "            print(\"âœ… Groupes de crimes crÃ©Ã©s\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Mapping crimes Ã©chouÃ©: {e}\")\n",
    "    \n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ca0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_crime_model():\n",
    "    \"\"\"Charge le meilleur modÃ¨le disponible\"\"\"\n",
    "    print(\"ðŸ¤– Chargement du meilleur modÃ¨le\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Chercher modÃ¨les dans l'ordre de prÃ©fÃ©rence\n",
    "    model_candidates = [\n",
    "        'stacking.joblib',      # Ensemble stacking\n",
    "        'voting_soft.joblib',   # Ensemble voting\n",
    "        'voting_hard.joblib',   # Ensemble voting hard\n",
    "        'random_forest_baseline.joblib',  # RF baseline\n",
    "        'xgboost_baseline.joblib',        # XGB baseline\n",
    "        'lightgbm_baseline.joblib'        # LGB baseline\n",
    "    ]\n",
    "    \n",
    "    model = None\n",
    "    model_name = None\n",
    "    \n",
    "    for candidate in model_candidates:\n",
    "        model_path = MODELS_DIR / candidate\n",
    "        if model_path.exists():\n",
    "            try:\n",
    "                model = joblib.load(model_path)\n",
    "                model_name = candidate.replace('.joblib', '')\n",
    "                print(f\"âœ… ModÃ¨le chargÃ©: {candidate}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Erreur chargement {candidate}: {e}\")\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"âš ï¸ Aucun modÃ¨le prÃ©-entraÃ®nÃ© trouvÃ© - crÃ©ation d'un RandomForest de dÃ©monstration\")\n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "        model_name = 'demo_random_forest'\n",
    "        print(\"âœ… ModÃ¨le de dÃ©monstration crÃ©Ã© (sera entraÃ®nÃ© sur les donnÃ©es)\")\n",
    "    \n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2674d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger donnÃ©es et modÃ¨le\n",
    "df_crime = load_preprocessed_crime_data()\n",
    "model, model_name = load_best_crime_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbacb29",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”„ PrÃ©paration des DonnÃ©es pour Ã‰valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_evaluation(df, model):\n",
    "    \"\"\"PrÃ©pare les donnÃ©es pour l'Ã©valuation du modÃ¨le\"\"\"\n",
    "    print(\"ðŸ“ PrÃ©paration des donnÃ©es pour Ã©valuation\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Features standard du projet\n",
    "    feature_cols = ['Hour', 'Day_of_week', 'Month_num', 'LAT', 'LON']\n",
    "    \n",
    "    # Ajouter features optionnelles si disponibles\n",
    "    optional_features = ['Vict Age', 'AREA']\n",
    "    for feat in optional_features:\n",
    "        if feat in df.columns:\n",
    "            feature_cols.append(feat)\n",
    "    \n",
    "    # Label\n",
    "    label_col = 'Crime_Group' if 'Crime_Group' in df.columns else 'Crm Cd Desc'\n",
    "    \n",
    "    # PrÃ©parer dataset ML\n",
    "    df_ml = df[feature_cols + [label_col]].dropna()\n",
    "    \n",
    "    X = df_ml[feature_cols]\n",
    "    y = df_ml[label_col]\n",
    "    \n",
    "    # Split (mÃªme seed que train.py)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Si le modÃ¨le n'est pas entraÃ®nÃ©, l'entraÃ®ner\n",
    "    if not hasattr(model, 'classes_') or model.classes_ is None:\n",
    "        print(\"ðŸš€ EntraÃ®nement du modÃ¨le sur les donnÃ©es...\")\n",
    "        \n",
    "        # Imputation si nÃ©cessaire (comme dans train.py)\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train_imputed = imputer.fit_transform(X_train)\n",
    "        X_test_imputed = imputer.transform(X_test)\n",
    "        \n",
    "        # EntraÃ®ner\n",
    "        model.fit(X_train_imputed, y_train)\n",
    "        \n",
    "        # Utiliser les donnÃ©es imputÃ©es pour la suite\n",
    "        X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns, index=X_train.index)\n",
    "        X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns, index=X_test.index)\n",
    "        \n",
    "        print(\"âœ… ModÃ¨le entraÃ®nÃ©\")\n",
    "    else:\n",
    "        # Imputation pour cohÃ©rence\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_test = pd.DataFrame(\n",
    "            imputer.fit_transform(X_test), \n",
    "            columns=X_test.columns, \n",
    "            index=X_test.index\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ… DonnÃ©es prÃ©parÃ©es:\")\n",
    "    print(f\"   Train: {X_train.shape[0]} Ã©chantillons\")\n",
    "    print(f\"   Test:  {X_test.shape[0]} Ã©chantillons\")\n",
    "    print(f\"   Features: {feature_cols}\")\n",
    "    print(f\"   Classes: {len(np.unique(y))}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols, label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrÃ©parer les donnÃ©es\n",
    "X_train, X_test, y_train, y_test, features, label_column = prepare_data_for_evaluation(df_crime, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6b51b",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ GÃ©nÃ©ration des PrÃ©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b86e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"GÃ©nÃ¨re les prÃ©dictions et probabilitÃ©s\"\"\"\n",
    "    print(\"ðŸŽ¯ GÃ©nÃ©ration des prÃ©dictions\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # PrÃ©dictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # ProbabilitÃ©s (si disponible)\n",
    "    y_proba_train = None\n",
    "    y_proba_test = None\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        try:\n",
    "            y_proba_train = model.predict_proba(X_train)\n",
    "            y_proba_test = model.predict_proba(X_test)\n",
    "            print(\"âœ… ProbabilitÃ©s gÃ©nÃ©rÃ©es\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ProbabilitÃ©s non disponibles: {e}\")\n",
    "    \n",
    "    # MÃ©triques de base\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    print(f\"âœ… Performances de base:\")\n",
    "    print(f\"   Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"   Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"   Test F1-Score:  {test_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'y_proba_train': y_proba_train,\n",
    "        'y_proba_test': y_proba_test,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af791a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GÃ©nÃ©rer les prÃ©dictions\n",
    "predictions = generate_predictions(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd5a65",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ† NIVEAU 3 : Ã‰VALUATION PERFORMANCE AVEC DEEPCHECKS\n",
    "\n",
    "### Suite d'Ã©valuation modÃ¨le adaptÃ©e au crime\n",
    "\n",
    "Les checks exÃ©cutÃ©s :\n",
    "1. **ConfusionMatrixReport** : Matrice de confusion dÃ©taillÃ©e\n",
    "2. **ModelErrorAnalysis** : Analyse des erreurs par segments\n",
    "3. **RocReport** : Courbes ROC par classe\n",
    "4. **CalibrationScore** : Calibration des probabilitÃ©s\n",
    "5. **SegmentPerformance** : Performance par sous-groupes\n",
    "6. **WeakSegmentsPerformance** : DÃ©tection de segments faibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deepchecks_datasets_with_predictions(X_train, X_test, y_train, y_test, predictions, features):\n",
    "    \"\"\"CrÃ©e les datasets Deepchecks avec prÃ©dictions\"\"\"\n",
    "    print(\"ðŸ“ CrÃ©ation des datasets Deepchecks avec prÃ©dictions\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # PrÃ©parer DataFrames avec labels et prÃ©dictions\n",
    "    train_df = X_train.copy()\n",
    "    train_df['__label__'] = y_train.values if hasattr(y_train, 'values') else list(y_train)\n",
    "    \n",
    "    test_df = X_test.copy()\n",
    "    test_df['__label__'] = y_test.values if hasattr(y_test, 'values') else list(y_test)\n",
    "    \n",
    "    # Features catÃ©gorielles\n",
    "    cat_features = []\n",
    "    for col in features:\n",
    "        if col in ['Day_of_week', 'Month_num', 'AREA'] or train_df[col].dtype == 'object':\n",
    "            cat_features.append(col)\n",
    "    \n",
    "    print(f\"âœ… Datasets avec prÃ©dictions crÃ©Ã©s\")\n",
    "    print(f\"   Features catÃ©gorielles: {cat_features}\")\n",
    "    \n",
    "    if DEEPCHECKS_AVAILABLE:\n",
    "        train_dataset = Dataset(\n",
    "            train_df, \n",
    "            label='__label__', \n",
    "            cat_features=cat_features\n",
    "        )\n",
    "        test_dataset = Dataset(\n",
    "            test_df, \n",
    "            label='__label__', \n",
    "            cat_features=cat_features\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "    else:\n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ddc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_performance_evaluation(train_ds, test_ds, model, predictions):\n",
    "    \"\"\"NIVEAU 3: Ã‰valuation complÃ¨te de performance du modÃ¨le\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ† NIVEAU 3: Ã‰VALUATION PERFORMANCE DU MODÃˆLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if DEEPCHECKS_AVAILABLE:\n",
    "        print(\"\\nðŸ” ExÃ©cution des checks Deepchecks:\")\n",
    "        print(\"   1. Confusion Matrix Report (matrices de confusion)\")\n",
    "        print(\"   2. Model Error Analysis (analyse des erreurs)\")\n",
    "        print(\"   3. ROC Report (courbes ROC)\")\n",
    "        print(\"   4. Calibration Score (calibration probabilitÃ©s)\")\n",
    "        print(\"   5. Segment Performance (performance par segments)\")\n",
    "        print(\"   6. Weak Segments Performance (segments faibles)\")\n",
    "        \n",
    "        # Suite d'Ã©valuation modÃ¨le\n",
    "        eval_suite = model_evaluation()\n",
    "        \n",
    "        print(\"\\nâ³ ExÃ©cution de l'Ã©valuation du modÃ¨le...\")\n",
    "        try:\n",
    "            result = eval_suite.run(train_ds, test_ds, model)\n",
    "            \n",
    "            # Sauvegarder le rapport\n",
    "            perf_report_path = REPORTS_DIR / f'crime_model_performance_{model_name}.html'\n",
    "            result.save_as_html(str(perf_report_path))\n",
    "            \n",
    "            print(f\"âœ… Rapport performance sauvegardÃ©: {perf_report_path.name}\")\n",
    "            results['deepchecks_result'] = result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur suite Deepchecks: {e}\")\n",
    "    \n",
    "    # Analyses manuelles de performance spÃ©cifiques au crime\n",
    "    print(\"\\nðŸ“Š Analyses Performance Manuelles SpÃ©cifiques Crime:\")\n",
    "    \n",
    "    # 1. Performance par classe de crime\n",
    "    y_test_vals = test_ds['__label__'] if isinstance(test_ds, pd.DataFrame) else test_ds.label\n",
    "    y_pred_vals = predictions['y_pred_test']\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "    class_report = classification_report(y_test_vals, y_pred_vals, output_dict=True)\n",
    "    \n",
    "    print(f\"\\n   ðŸ“‹ Performance par classe:\")\n",
    "    for class_name, metrics in class_report.items():\n",
    "        if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "            f1 = metrics['f1-score']\n",
    "            support = metrics['support']\n",
    "            status = \"âœ…\" if f1 > 0.7 else \"âš ï¸\" if f1 > 0.5 else \"ðŸ”´\"\n",
    "            print(f\"      {class_name}: F1={f1:.3f} Support={support} {status}\")\n",
    "    \n",
    "    # 2. Matrice de confusion dÃ©taillÃ©e\n",
    "    cm = confusion_matrix(y_test_vals, y_pred_vals)\n",
    "    classes = np.unique(np.concatenate([y_test_vals, y_pred_vals]))\n",
    "    \n",
    "    print(f\"\\n   ðŸ“Š Erreurs de classification principales:\")\n",
    "    for i, true_class in enumerate(classes):\n",
    "        for j, pred_class in enumerate(classes):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                error_rate = cm[i, j] / cm[i, :].sum()\n",
    "                if error_rate > 0.1:  # Plus de 10% d'erreur\n",
    "                    print(f\"      {true_class} â†’ {pred_class}: {cm[i,j]} erreurs ({error_rate:.1%})\")\n",
    "    \n",
    "    # 3. Analyse des probabilitÃ©s (si disponible)\n",
    "    if predictions['y_proba_test'] is not None:\n",
    "        proba_max = np.max(predictions['y_proba_test'], axis=1)\n",
    "        confidence_mean = np.mean(proba_max)\n",
    "        low_confidence = (proba_max < 0.6).sum()\n",
    "        \n",
    "        print(f\"\\n   ðŸŽ¯ Analyse de confiance:\")\n",
    "        print(f\"      Confiance moyenne: {confidence_mean:.3f}\")\n",
    "        print(f\"      PrÃ©dictions peu sÃ»res (<60%): {low_confidence} ({low_confidence/len(proba_max):.1%})\")\n",
    "        \n",
    "        results['confidence_stats'] = {\n",
    "            'mean_confidence': confidence_mean,\n",
    "            'low_confidence_pct': low_confidence/len(proba_max)\n",
    "        }\n",
    "    \n",
    "    results['classification_report'] = class_report\n",
    "    results['confusion_matrix'] = cm\n",
    "    results['classes'] = classes\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a571b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les datasets avec prÃ©dictions\n",
    "train_dataset, test_dataset = create_deepchecks_datasets_with_predictions(\n",
    "    X_train, X_test, y_train, y_test, predictions, features\n",
    ")\n",
    "\n",
    "# ExÃ©cuter l'Ã©valuation de performance\n",
    "performance_results = run_model_performance_evaluation(\n",
    "    train_dataset, test_dataset, model, predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3abc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le widget interactif Deepchecks (si disponible)\n",
    "if 'deepchecks_result' in performance_results:\n",
    "    print(\"ðŸ“Š Rapport interactif Deepchecks:\")\n",
    "    performance_results['deepchecks_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c430c",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ” Analyse SpÃ©cialisÃ©e Crime LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e432344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialized_crime_analysis(model, X_test, y_test, predictions, features):\n",
    "    \"\"\"Analyses spÃ©cialisÃ©es pour la prÃ©diction de crimes LA\"\"\"\n",
    "    print(\"\\nðŸ” ANALYSES SPÃ‰CIALISÃ‰ES CRIME LA\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    y_pred = predictions['y_pred_test']\n",
    "    \n",
    "    # 1. Performance par zone gÃ©ographique (si AREA disponible)\n",
    "    if 'AREA' in X_test.columns:\n",
    "        print(\"\\nðŸ“ Performance par zone LAPD:\")\n",
    "        \n",
    "        areas = X_test['AREA'].unique()[:5]  # Top 5 zones\n",
    "        for area in sorted(areas):\n",
    "            mask = X_test['AREA'] == area\n",
    "            if mask.sum() > 10:  # Au moins 10 Ã©chantillons\n",
    "                area_acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "                status = \"âœ…\" if area_acc > 0.7 else \"âš ï¸\" if area_acc > 0.5 else \"ðŸ”´\"\n",
    "                print(f\"      Zone {area}: Accuracy={area_acc:.3f} ({mask.sum()} crimes) {status}\")\n",
    "    \n",
    "    # 2. Performance par pÃ©riode temporelle\n",
    "    if 'Hour' in X_test.columns:\n",
    "        print(\"\\nðŸ• Performance par pÃ©riode:\")\n",
    "        \n",
    "        # Regrouper par pÃ©riodes\n",
    "        X_test_copy = X_test.copy()\n",
    "        X_test_copy['period'] = pd.cut(\n",
    "            X_test_copy['Hour'], \n",
    "            bins=[0, 6, 12, 18, 24], \n",
    "            labels=['Nuit (0-6h)', 'Matin (6-12h)', 'AprÃ¨s-midi (12-18h)', 'SoirÃ©e (18-24h)'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        for period in X_test_copy['period'].cat.categories:\n",
    "            mask = X_test_copy['period'] == period\n",
    "            if mask.sum() > 10:\n",
    "                period_acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "                status = \"âœ…\" if period_acc > 0.7 else \"âš ï¸\" if period_acc > 0.5 else \"ðŸ”´\"\n",
    "                print(f\"      {period}: Accuracy={period_acc:.3f} ({mask.sum()} crimes) {status}\")\n",
    "    \n",
    "    # 3. Analyse des erreurs critiques (crimes violents mal classÃ©s)\n",
    "    print(\"\\nðŸš¨ Analyse erreurs critiques:\")\n",
    "    \n",
    "    # Identifier les crimes violents\n",
    "    violent_crimes_mask = (y_test == 'Violent Crime') if 'Violent Crime' in y_test.values else pd.Series([False] * len(y_test))\n",
    "    \n",
    "    if violent_crimes_mask.sum() > 0:\n",
    "        violent_recall = recall_score(y_test, y_pred, labels=['Violent Crime'], average=None)[0] if 'Violent Crime' in y_test.values else 0\n",
    "        violent_precision = precision_score(y_test, y_pred, labels=['Violent Crime'], average=None)[0] if 'Violent Crime' in y_test.values else 0\n",
    "        \n",
    "        print(f\"      Crimes violents - Recall: {violent_recall:.3f} {'âœ…' if violent_recall > 0.8 else 'ðŸ”´ CRITIQUE'}\")\n",
    "        print(f\"      Crimes violents - Precision: {violent_precision:.3f} {'âœ…' if violent_precision > 0.7 else 'âš ï¸'}\")\n",
    "        \n",
    "        # Crimes violents manquÃ©s\n",
    "        missed_violent = violent_crimes_mask & (y_pred != 'Violent Crime')\n",
    "        if missed_violent.sum() > 0:\n",
    "            print(f\"      âš ï¸ {missed_violent.sum()} crimes violents non dÃ©tectÃ©s\")\n",
    "    \n",
    "    # 4. Feature importance (si disponible)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\nðŸ“Š Importance des features:\")\n",
    "        \n",
    "        importances = model.feature_importances_\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        for _, row in feature_imp.head().iterrows():\n",
    "            print(f\"      {row['feature']}: {row['importance']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'violent_crime_performance': {\n",
    "            'recall': violent_recall if 'violent_recall' in locals() else None,\n",
    "            'precision': violent_precision if 'violent_precision' in locals() else None\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExÃ©cuter les analyses spÃ©cialisÃ©es\n",
    "specialized_results = specialized_crime_analysis(model, X_test, y_test, predictions, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32033ee",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š RÃ©sumÃ© Final et Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… NIVEAU 3 : Ã‰VALUATION PERFORMANCE - TERMINÃ‰\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ† RÃ©sumÃ© Performance Globale:\")\n",
    "print(f\"   ModÃ¨le Ã©valuÃ©: {model_name}\")\n",
    "print(f\"   Test Accuracy: {predictions['test_acc']:.4f}\")\n",
    "print(f\"   Test F1-Score: {predictions['test_f1']:.4f}\")\n",
    "print(f\"   Ã‰chantillons test: {len(y_test)}\")\n",
    "\n",
    "# Ã‰valuation qualitative\n",
    "if predictions['test_acc'] > 0.85:\n",
    "    quality = \"ðŸ† EXCELLENTE\"\n",
    "elif predictions['test_acc'] > 0.75:\n",
    "    quality = \"âœ… BONNE\"\n",
    "elif predictions['test_acc'] > 0.65:\n",
    "    quality = \"âš ï¸ ACCEPTABLE\"\n",
    "else:\n",
    "    quality = \"ðŸ”´ INSUFFISANTE\"\n",
    "\n",
    "print(f\"   QualitÃ© globale: {quality}\")\n",
    "\n",
    "# Performance par classe\n",
    "if 'classification_report' in performance_results:\n",
    "    print(f\"\\nðŸ“Š Performance par classe de crime:\")\n",
    "    class_report = performance_results['classification_report']\n",
    "    for class_name, metrics in class_report.items():\n",
    "        if isinstance(metrics, dict) and 'f1-score' in metrics:\n",
    "            f1 = metrics['f1-score']\n",
    "            status = \"âœ…\" if f1 > 0.7 else \"âš ï¸\" if f1 > 0.5 else \"ðŸ”´\"\n",
    "            print(f\"   {class_name}: F1={f1:.3f} {status}\")\n",
    "\n",
    "# Confiance du modÃ¨le\n",
    "if 'confidence_stats' in performance_results:\n",
    "    conf_stats = performance_results['confidence_stats']\n",
    "    print(f\"\\nðŸŽ¯ Analyse de Confiance:\")\n",
    "    print(f\"   Confiance moyenne: {conf_stats['mean_confidence']:.3f}\")\n",
    "    print(f\"   PrÃ©dictions incertaines: {conf_stats['low_confidence_pct']:.1%}\")\n",
    "\n",
    "# Recommandations\n",
    "print(f\"\\nðŸ’¡ Recommandations:\")\n",
    "\n",
    "if predictions['test_acc'] > 0.8:\n",
    "    print(f\"   âœ… ModÃ¨le prÃªt pour la production\")\n",
    "    print(f\"   ðŸ“ˆ ConsidÃ©rer le dÃ©ploiement via API\")\n",
    "    print(f\"   ðŸ”„ Mettre en place monitoring continu\")\n",
    "else:\n",
    "    print(f\"   ðŸ”§ AmÃ©liorer le modÃ¨le avant production:\")\n",
    "    print(f\"      - Essayer d'autres algorithmes (ensemble methods)\")\n",
    "    print(f\"      - Ajouter plus de features\")\n",
    "    print(f\"      - Optimiser les hyperparamÃ¨tres\")\n",
    "    print(f\"      - Ã‰quilibrer mieux les classes\")\n",
    "\n",
    "# Analyses spÃ©cialisÃ©es\n",
    "if specialized_results['violent_crime_performance']['recall']:\n",
    "    violent_recall = specialized_results['violent_crime_performance']['recall']\n",
    "    if violent_recall < 0.8:\n",
    "        print(f\"   ðŸš¨ CRITIQUE: AmÃ©liorer dÃ©tection crimes violents (recall={violent_recall:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Rapports gÃ©nÃ©rÃ©s:\")\n",
    "for report_file in REPORTS_DIR.glob(f'*{model_name}*.html'):\n",
    "    print(f\"   ðŸ“„ {report_file.name}\")\n",
    "\n",
    "print(f\"\\nðŸ”„ Pipeline Deepchecks complet:\")\n",
    "print(f\"   1. âœ… NIVEAU 1: IntÃ©gritÃ© des donnÃ©es\")\n",
    "print(f\"   2. âœ… NIVEAU 2: Drift et distribution\")\n",
    "print(f\"   3. âœ… NIVEAU 3: Performance du modÃ¨le\")\n",
    "print(f\"   âž¡ï¸ PrÃªt pour la mise en production !\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
