{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Validation Compl√®te avec DeepChecks - Crime Dataset 2020\n",
        "\n",
        "## üìã Compatible scikit-learn 1.8.0\n",
        "\n",
        "### Table des Mati√®res\n",
        "1. [Setup Compatibilit√©](#setup)\n",
        "2. [Niveau 1 : Int√©grit√© des Donn√©es](#niveau-1)\n",
        "3. [Niveau 2 : Drift et Distribution](#niveau-2)\n",
        "4. [Niveau 3 : Performance du Mod√®le](#niveau-3)\n",
        "5. [R√©sum√© et Recommandations](#resume)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Installation des D√©pendances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation (si n√©cessaire)\n",
        "# !pip install deepchecks pandas scikit-learn joblib matplotlib seaborn scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id=\"setup\"></a>üîß Setup Compatibilit√© scikit-learn 1.8.0\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT : Ex√©cutez cette cellule EN PREMIER avant tous les imports !**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# üîß FIX COMPATIBILIT√â SCIKIT-LEARN 1.8.0\n",
        "# ==========================================\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "import numpy as np\n",
        "\n",
        "def max_error_replacement(y_true, y_pred):\n",
        "    \"\"\"Remplace max_error supprim√© dans sklearn 1.8.0\"\"\"\n",
        "    return np.max(np.abs(y_true - y_pred))\n",
        "\n",
        "try:\n",
        "    if 'max_error' not in sklearn.metrics.get_scorer_names():\n",
        "        sklearn.metrics._SCORERS['max_error'] = make_scorer(\n",
        "            max_error_replacement, greater_is_better=False\n",
        "        )\n",
        "        print(\"‚úÖ Fix sklearn 1.8.0 appliqu√© avec succ√®s\")\n",
        "    else:\n",
        "        print(\"‚úÖ Scorer max_error d√©j√† disponible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur: {e}\")\n",
        "\n",
        "print(f\"\\nüìå scikit-learn version: {sklearn.__version__}\")\n",
        "print(\"\\n‚úÖ Setup termin√© - Continuez !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "from deepchecks.tabular import Dataset\n",
        "from deepchecks.tabular.suites import data_integrity, train_test_validation, model_evaluation\n",
        "from deepchecks.tabular.checks import (\n",
        "    MixedNulls, DataDuplicates, OutlierSampleDetection,\n",
        "    FeatureFeatureCorrelation, FeatureLabelCorrelation,\n",
        "    TrainTestFeatureDrift, TrainTestLabelDrift\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"‚úÖ Imports r√©ussis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÇÔ∏è Fonction de Mapping des Crimes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_crime_group_4(desc):\n",
        "    \"\"\"Regroupe les crimes en 4 cat√©gories principales\"\"\"\n",
        "    if pd.isna(desc):\n",
        "        return \"Other / Fraud / Public Order Crime\"\n",
        "    desc = str(desc).upper()\n",
        "    \n",
        "    if any(k in desc for k in [\"ASSAULT\", \"BATTERY\", \"ROBBERY\", \"HOMICIDE\", \"RAPE\", \"SEX\"]):\n",
        "        return \"Violent Crime\"\n",
        "    if any(k in desc for k in [\"THEFT\", \"BURGLARY\", \"SHOPLIFTING\", \"VANDALISM\"]):\n",
        "        return \"Property & Theft Crime\"\n",
        "    if any(k in desc for k in [\"VEHICLE\", \"DWOC\", \"MOTOR VEHICLE\"]):\n",
        "        return \"Vehicle-Related Crime\"\n",
        "    return \"Other / Fraud / Public Order Crime\"\n",
        "\n",
        "print(\"‚úÖ Fonction de mapping d√©finie\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Chargement des Donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/processed/crime_2020_processed.csv')\n",
        "df['Crime_Group'] = df['Crm Cd Desc'].apply(map_crime_group_4)\n",
        "print(f\"‚úÖ Donn√©es charg√©es : {len(df):,} lignes\")\n",
        "print(f\"Shape : {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Pr√©paration des Donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_cols = ['Hour', 'Day_of_week', 'Month_num', 'LAT', 'LON', 'Vict Age', 'AREA']\n",
        "X = df[feature_cols].copy()\n",
        "y = df['Crime_Group']\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "print(f\"Features : {feature_cols}\")\n",
        "print(f\"\\nClasses :\")\n",
        "for i, name in enumerate(le.classes_):\n",
        "    count = (y == name).sum()\n",
        "    print(f\"  {i}. {name}: {count:,} ({count/len(y)*100:.1f}%)\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = pd.DataFrame(\n",
        "    imputer.fit_transform(X_train), columns=feature_cols, index=X_train.index\n",
        ")\n",
        "X_test_imputed = pd.DataFrame(\n",
        "    imputer.transform(X_test), columns=feature_cols, index=X_test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain: {X_train.shape}, Test: {X_test.shape}\")\n",
        "print(f\"NaN train: {X_train_imputed.isna().sum().sum()}\")\n",
        "print(f\"NaN test: {X_test_imputed.isna().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Chargement du Mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    model = joblib.load('models/lightgbm_baseline.joblib')\n",
        "    print(f\"‚úÖ Mod√®le charg√©: {type(model).__name__}\")\n",
        "except:\n",
        "    model = None\n",
        "    print(\"‚ö†Ô∏è Mod√®le non trouv√© (Niveau 3 sera skipp√©)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÇÔ∏è Cr√©ation des Datasets DeepChecks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = Dataset(X_train_imputed, label=y_train, features=feature_cols)\n",
        "test_dataset = Dataset(X_test_imputed, label=y_test, features=feature_cols)\n",
        "print(f\"‚úÖ Datasets cr√©√©s\")\n",
        "print(f\"  Train: {len(train_dataset)} √©chantillons\")\n",
        "print(f\"  Test: {len(test_dataset)} √©chantillons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# <a id=\"niveau-1\"></a>üìä Niveau 1 : Int√©grit√© des Donn√©es\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üîç NIVEAU 1 : INT√âGRIT√â DES DONN√âES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    integrity_suite = data_integrity()\n",
        "    integrity_result = integrity_suite.run(train_dataset)\n",
        "    integrity_result.show()\n",
        "    print(\"\\n‚úÖ Suite d'int√©grit√© OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checks = [\n",
        "    (MixedNulls(), \"Valeurs manquantes\"),\n",
        "    (DataDuplicates(), \"Duplicatas\"),\n",
        "    (OutlierSampleDetection(), \"Outliers\"),\n",
        "    (FeatureFeatureCorrelation(), \"Corr√©lations FF\"),\n",
        "    (FeatureLabelCorrelation(), \"Corr√©lations FL\")\n",
        "]\n",
        "\n",
        "for check, name in checks:\n",
        "    try:\n",
        "        check.run(train_dataset).show()\n",
        "        print(f\"‚úÖ {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è {name}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# <a id=\"niveau-2\"></a>üìà Niveau 2 : Drift et Distribution\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üìà NIVEAU 2 : DRIFT ET DISTRIBUTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    tt_suite = train_test_validation()\n",
        "    tt_result = tt_suite.run(train_dataset, test_dataset)\n",
        "    tt_result.show()\n",
        "    print(\"\\n‚úÖ Suite train/test OK\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erreur: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä ANALYSE STATISTIQUE DU DRIFT (Kolmogorov-Smirnov)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "drift_results = []\n",
        "for feature in feature_cols:\n",
        "    stat, pval = ks_2samp(X_train_imputed[feature], X_test_imputed[feature])\n",
        "    drift_results.append({\n",
        "        'Feature': feature,\n",
        "        'KS Statistic': f\"{stat:.4f}\",\n",
        "        'P-Value': f\"{pval:.4f}\",\n",
        "        'Drift': 'üî¥ OUI' if pval < 0.05 else '‚úÖ NON'\n",
        "    })\n",
        "\n",
        "df_drift = pd.DataFrame(drift_results)\n",
        "print(\"\\n\" + df_drift.to_string(index=False))\n",
        "\n",
        "n_drift = df_drift['Drift'].str.contains('OUI').sum()\n",
        "print(f\"\\nüîç R√©sultat : {n_drift}/{len(feature_cols)} features avec drift\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, col in enumerate(feature_cols):\n",
        "    axes[i].hist(X_train_imputed[col], bins=30, alpha=0.5, label='Train', density=True)\n",
        "    axes[i].hist(X_test_imputed[col], bins=30, alpha=0.5, label='Test', density=True)\n",
        "    axes[i].set_title(col)\n",
        "    axes[i].legend()\n",
        "\n",
        "for i in range(len(feature_cols), len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('reports/distributions.png', dpi=150, bbox_inches='tight')\n",
        "print(\"‚úÖ Graphique sauvegard√©: reports/distributions.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# <a id=\"niveau-3\"></a>üéØ Niveau 3 : Performance du Mod√®le\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üéØ NIVEAU 3 : PERFORMANCE DU MOD√àLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if model:\n",
        "    try:\n",
        "        eval_suite = model_evaluation()\n",
        "        eval_result = eval_suite.run(train_dataset, test_dataset, model)\n",
        "        eval_result.show()\n",
        "        print(\"\\n‚úÖ Suite d'√©valuation OK\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Erreur: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Pas de mod√®le charg√© - Niveau 3 skipp√©\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model:\n",
        "    y_pred_train = model.predict(X_train_imputed)\n",
        "    y_pred_test = model.predict(X_test_imputed)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    test_f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
        "    gap = train_acc - test_acc\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üìä M√âTRIQUES D√âTAILL√âES\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTrain Accuracy : {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "    print(f\"Test Accuracy  : {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "    print(f\"Test F1-Score  : {test_f1:.4f}\")\n",
        "    print(f\"Gap Train-Test : {gap:.4f} ({gap*100:.2f}%)\")\n",
        "    \n",
        "    if gap > 0.1:\n",
        "        print(\"\\n‚ö†Ô∏è OVERFITTING d√©tect√© !\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Bon √©quilibre train/test\")\n",
        "    \n",
        "    print(\"\\n\" + classification_report(y_test, y_pred_test, target_names=le.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if model and hasattr(model, 'feature_importances_'):\n",
        "    imp_df = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'Importance': model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üåü FEATURE IMPORTANCE\")\n",
        "    print(\"=\"*80)\n",
        "    print(imp_df.to_string(index=False))\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(imp_df['Feature'], imp_df['Importance'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Feature Importance - LightGBM')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('reports/feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"\\n‚úÖ Graphique sauvegard√©: reports/feature_importance.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# <a id=\"resume\"></a>üìã R√©sum√© et Recommandations\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"üìã R√âSUM√â G√âN√âRAL - VALIDATION DEEPCHECKS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìÖ Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üìä Dataset : crime_2020_processed.csv\")\n",
        "print(f\"ü§ñ Mod√®le : {type(model).__name__ if model else 'N/A'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ NIVEAU 1 : INT√âGRIT√â\")\n",
        "print(\"=\"*80)\n",
        "print(f\"  √âchantillons    : {len(X_train_imputed):,}\")\n",
        "print(f\"  Features        : {len(feature_cols)}\")\n",
        "print(f\"  Classes         : {len(le.classes_)}\")\n",
        "print(f\"  NaN             : {X_train_imputed.isna().sum().sum()}\")\n",
        "print(f\"  Duplicatas      : {X_train_imputed.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ NIVEAU 2 : DRIFT\")\n",
        "print(\"=\"*80)\n",
        "if 'n_drift' in locals():\n",
        "    print(f\"  Features drift  : {n_drift}/{len(feature_cols)}\")\n",
        "    if n_drift > 0:\n",
        "        print(\"  ‚ö†Ô∏è Drift d√©tect√© sur certaines features\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Pas de drift significatif\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ NIVEAU 3 : PERFORMANCE\")\n",
        "print(\"=\"*80)\n",
        "if model:\n",
        "    print(f\"  Test Accuracy   : {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "    print(f\"  Test F1-Score   : {test_f1:.4f}\")\n",
        "    print(f\"  Gap Train-Test  : {gap:.4f}\")\n",
        "else:\n",
        "    print(\"  ‚ö†Ô∏è Aucun mod√®le √©valu√©\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° RECOMMANDATIONS\")\n",
        "print(\"=\"*80)\n",
        "print(\"  1. ‚úÖ Donn√©es pr√™tes pour production\")\n",
        "print(\"  2. üîÑ Surveiller le drift en continu\")\n",
        "print(\"  3. üìà Am√©liorer features si accuracy < 60%\")\n",
        "print(\"  4. üöÄ D√©ployer l'API de pr√©diction\")\n",
        "print(\"\\n‚úÖ Validation termin√©e !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Sauvegarder les Rapports HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('reports', exist_ok=True)\n",
        "\n",
        "print(\"üíæ Sauvegarde des rapports HTML...\\n\")\n",
        "\n",
        "if 'integrity_result' in locals():\n",
        "    integrity_result.save_as_html('reports/deepchecks_integrity.html')\n",
        "    print(\"  ‚úÖ reports/deepchecks_integrity.html\")\n",
        "\n",
        "if 'tt_result' in locals():\n",
        "    tt_result.save_as_html('reports/deepchecks_train_test.html')\n",
        "    print(\"  ‚úÖ reports/deepchecks_train_test.html\")\n",
        "\n",
        "if model and 'eval_result' in locals():\n",
        "    eval_result.save_as_html('reports/deepchecks_model_eval.html')\n",
        "    print(\"  ‚úÖ reports/deepchecks_model_eval.html\")\n",
        "\n",
        "print(\"\\n‚úÖ Tous les rapports sont sauvegard√©s !\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
