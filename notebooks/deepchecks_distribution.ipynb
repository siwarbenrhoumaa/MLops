{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e42920",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ DEEPCHECKS CRIME LA - NIVEAU 2 : DRIFT ET DISTRIBUTION\n",
    "\n",
    "## ğŸ¯ Objectif\n",
    "Analyser les distributions et dÃ©tecter le drift entre train/test pour les donnÃ©es de criminalitÃ© :\n",
    "- VÃ©rification de la stabilitÃ© des features entre train/test\n",
    "- DÃ©tection de drift dans les labels (types de crimes)\n",
    "- Analyse des propriÃ©tÃ©s statistiques des donnÃ©es\n",
    "- Validation de la reprÃ©sentativitÃ© du split\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76364b",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ajouter src au path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "print(\"âœ… Imports de base effectuÃ©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepchecks Tabular pour drift\n",
    "try:\n",
    "    from deepchecks.tabular import Dataset\n",
    "    from deepchecks.tabular.suites import train_test_validation\n",
    "    from deepchecks.tabular.checks import (\n",
    "        TrainTestFeatureDrift, TrainTestLabelDrift,\n",
    "        WholeDatasetDrift, FeatureLabelCorrelation,\n",
    "        TrainTestSamplesMix, NewLabelTrainTest,\n",
    "        CategoryMismatchTrainTest, DateTrainTestLeakageDuplicates\n",
    "    )\n",
    "    DEEPCHECKS_AVAILABLE = True\n",
    "    print(\"âœ… Deepchecks importÃ©\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Deepchecks non disponible: {e}\")\n",
    "    DEEPCHECKS_AVAILABLE = False\n",
    "\n",
    "# ML pour le split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "REPORTS_DIR = BASE_DIR / 'reports' / 'deepchecks'\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“ˆ DEEPCHECKS CRIME LA - NIVEAU 2 : DRIFT ET DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ“ Base: {BASE_DIR}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d6066",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“¥ Chargement et PrÃ©paration des DonnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_crime_data():\n",
    "    \"\"\"Charge et prÃ©pare les donnÃ©es avec preprocessing si nÃ©cessaire\"\"\"\n",
    "    print(\"ğŸ“¦ Chargement et prÃ©paration des donnÃ©es\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Chercher donnÃ©es processed en prioritÃ©\n",
    "    processed_files = list((DATA_DIR / 'processed').glob('*.csv'))\n",
    "    raw_files = list((DATA_DIR / 'raw').glob('*.csv'))\n",
    "    \n",
    "    if processed_files:\n",
    "        data_file = processed_files[0]\n",
    "        df = pd.read_csv(data_file)\n",
    "        print(f\"âœ… DonnÃ©es processed chargÃ©es: {data_file.name}\")\n",
    "    elif raw_files:\n",
    "        data_file = raw_files[0]\n",
    "        df = pd.read_csv(data_file)\n",
    "        print(f\"âœ… DonnÃ©es raw chargÃ©es: {data_file.name}\")\n",
    "        \n",
    "        # Appliquer preprocessing basique si nÃ©cessaire\n",
    "        try:\n",
    "            from src.data.preprocessing import clean_data\n",
    "            df = clean_data(df)\n",
    "            print(\"âœ… Preprocessing appliquÃ©\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Preprocessing automatique Ã©chouÃ©: {e}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Aucun fichier CSV trouvÃ© dans data/\")\n",
    "    \n",
    "    print(f\"   Shape finale: {df.shape}\")\n",
    "    print(f\"   Colonnes: {df.columns.tolist()[:10]}{'...' if len(df.columns) > 10 else ''}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5aa578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donnÃ©es\n",
    "df_crime = load_and_prepare_crime_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5c7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”„ CrÃ©ation du Split Train/Test pour Drift Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ae2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split_for_drift(df):\n",
    "    \"\"\"CrÃ©e un split train/test reprÃ©sentatif pour analyse de drift\"\"\"\n",
    "    print(\"ğŸ“ CrÃ©ation du split Train/Test pour drift analysis\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Identifier label et features\n",
    "    label_col = None\n",
    "    if 'Crime_Group' in df.columns:\n",
    "        label_col = 'Crime_Group'\n",
    "    elif 'Crm Cd Desc' in df.columns:\n",
    "        # Mapper vers groupes si pas encore fait\n",
    "        df = df.copy()\n",
    "        try:\n",
    "            from src.models.train import map_crime_group_4\n",
    "            df['Crime_Group'] = df['Crm Cd Desc'].apply(map_crime_group_4)\n",
    "            label_col = 'Crime_Group'\n",
    "        except:\n",
    "            label_col = 'Crm Cd Desc'\n",
    "    \n",
    "    # Features pour ML\n",
    "    feature_cols = []\n",
    "    potential_features = ['Hour', 'Day_of_week', 'Month_num', 'LAT', 'LON', 'Vict Age', 'AREA']\n",
    "    \n",
    "    for col in potential_features:\n",
    "        if col in df.columns:\n",
    "            feature_cols.append(col)\n",
    "    \n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"Aucune feature ML trouvÃ©e. VÃ©rifiez le preprocessing.\")\n",
    "    \n",
    "    # PrÃ©parer les donnÃ©es pour split\n",
    "    df_ml = df[feature_cols + [label_col]].dropna()\n",
    "    \n",
    "    X = df_ml[feature_cols]\n",
    "    y = df_ml[label_col]\n",
    "    \n",
    "    # Split stratifiÃ©\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Split effectuÃ©:\")\n",
    "    print(f\"   Train: {X_train.shape[0]} Ã©chantillons\")\n",
    "    print(f\"   Test:  {X_test.shape[0]} Ã©chantillons\")\n",
    "    print(f\"   Features: {feature_cols}\")\n",
    "    print(f\"   Label: {label_col}\")\n",
    "    \n",
    "    # VÃ©rification distribution\n",
    "    train_dist = y_train.value_counts(normalize=True).sort_index()\n",
    "    test_dist = y_test.value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Distribution des labels:\")\n",
    "    for label in train_dist.index:\n",
    "        print(f\"   {label}: Train {train_dist[label]:.3f} | Test {test_dist[label]:.3f}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_cols, label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er le split\n",
    "X_train, X_test, y_train, y_test, features, label_column = create_train_test_split_for_drift(df_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca43bf8",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š CrÃ©ation des Datasets Deepchecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deepchecks_datasets(X_train, X_test, y_train, y_test, features):\n",
    "    \"\"\"CrÃ©e les datasets Deepchecks pour train et test\"\"\"\n",
    "    print(\"ğŸ“ CrÃ©ation des datasets Deepchecks\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Reconstituer DataFrames avec labels\n",
    "    train_df = X_train.copy()\n",
    "    train_df['__label__'] = y_train.values if hasattr(y_train, 'values') else list(y_train)\n",
    "    \n",
    "    test_df = X_test.copy()\n",
    "    test_df['__label__'] = y_test.values if hasattr(y_test, 'values') else list(y_test)\n",
    "    \n",
    "    # Identifier features catÃ©gorielles\n",
    "    cat_features = []\n",
    "    for col in features:\n",
    "        if train_df[col].dtype == 'object' or col in ['Day_of_week', 'Month_num', 'AREA']:\n",
    "            cat_features.append(col)\n",
    "    \n",
    "    print(f\"âœ… Datasets crÃ©Ã©s:\")\n",
    "    print(f\"   Features numÃ©riques: {[f for f in features if f not in cat_features]}\")\n",
    "    print(f\"   Features catÃ©gorielles: {cat_features}\")\n",
    "    \n",
    "    if DEEPCHECKS_AVAILABLE:\n",
    "        train_dataset = Dataset(\n",
    "            train_df, \n",
    "            label='__label__', \n",
    "            cat_features=cat_features,\n",
    "            index_name=None\n",
    "        )\n",
    "        test_dataset = Dataset(\n",
    "            test_df, \n",
    "            label='__label__', \n",
    "            cat_features=cat_features,\n",
    "            index_name=None\n",
    "        )\n",
    "        return train_dataset, test_dataset\n",
    "    else:\n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er les datasets\n",
    "train_dataset, test_dataset = create_deepchecks_datasets(X_train, X_test, y_train, y_test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6101f5",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ˆ NIVEAU 2 : ANALYSE DRIFT ET DISTRIBUTION\n",
    "\n",
    "### Suite de validation train/test adaptÃ©e au crime\n",
    "\n",
    "Les checks exÃ©cutÃ©s :\n",
    "1. **TrainTestFeatureDrift** : DÃ©tecte le drift dans les features entre train et test\n",
    "2. **TrainTestLabelDrift** : VÃ©rifie le drift dans la distribution des labels\n",
    "3. **WholeDatasetDrift** : Analyse globale du drift du dataset\n",
    "4. **FeatureLabelCorrelation** : CorrÃ©lation features/labels cohÃ©rente\n",
    "5. **TrainTestSamplesMix** : DÃ©tecte un mÃ©lange problÃ©matique train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd260ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_drift_analysis(train_ds, test_ds, train_df, test_df):\n",
    "    \"\"\"NIVEAU 2: Analyse de drift et distribution train/test\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“ˆ NIVEAU 2: ANALYSE DRIFT ET DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    if DEEPCHECKS_AVAILABLE:\n",
    "        print(\"\\nğŸ” ExÃ©cution des checks Deepchecks:\")\n",
    "        print(\"   1. Train-Test Feature Drift (drift des features)\")\n",
    "        print(\"   2. Train-Test Label Drift (drift des labels)\")\n",
    "        print(\"   3. Whole Dataset Drift (drift global)\")\n",
    "        print(\"   4. Feature-Label Correlation (corrÃ©lations)\")\n",
    "        print(\"   5. Train-Test Samples Mix (fuites de donnÃ©es)\")\n",
    "        \n",
    "        # Suite de validation train-test\n",
    "        validation_suite = train_test_validation()\n",
    "        \n",
    "        print(\"\\nâ³ ExÃ©cution de l'analyse de drift...\")\n",
    "        result = validation_suite.run(train_ds, test_ds)\n",
    "        \n",
    "        # Sauvegarder le rapport\n",
    "        drift_report_path = REPORTS_DIR / 'crime_drift_analysis_report.html'\n",
    "        result.save_as_html(str(drift_report_path))\n",
    "        \n",
    "        print(f\"âœ… Rapport de drift sauvegardÃ©: {drift_report_path.name}\")\n",
    "        \n",
    "        results['deepchecks_result'] = result\n",
    "    \n",
    "    # Analyses manuelles de drift spÃ©cifiques au crime\n",
    "    print(\"\\nğŸ“Š Analyses Drift Manuelles SpÃ©cifiques Crime:\")\n",
    "    \n",
    "    # 1. Distribution temporelle (heures)\n",
    "    if 'Hour' in train_df.columns:\n",
    "        from scipy.stats import ks_2samp\n",
    "        \n",
    "        ks_stat, p_value = ks_2samp(train_df['Hour'], test_df['Hour'])\n",
    "        print(f\"   Distribution heures - KS test: p={p_value:.4f} {'âœ… OK' if p_value > 0.05 else 'âš ï¸ DRIFT'}\")\n",
    "        results['hour_drift_p'] = p_value\n",
    "    \n",
    "    # 2. Distribution gÃ©ographique (coordonnÃ©es)\n",
    "    if 'LAT' in train_df.columns and 'LON' in train_df.columns:\n",
    "        # Test sur latitude\n",
    "        lat_ks_stat, lat_p = ks_2samp(train_df['LAT'].dropna(), test_df['LAT'].dropna())\n",
    "        # Test sur longitude\n",
    "        lon_ks_stat, lon_p = ks_2samp(train_df['LON'].dropna(), test_df['LON'].dropna())\n",
    "        \n",
    "        print(f\"   Distribution LAT - KS test: p={lat_p:.4f} {'âœ… OK' if lat_p > 0.05 else 'âš ï¸ DRIFT'}\")\n",
    "        print(f\"   Distribution LON - KS test: p={lon_p:.4f} {'âœ… OK' if lon_p > 0.05 else 'âš ï¸ DRIFT'}\")\n",
    "        \n",
    "        results['geo_drift'] = {'lat_p': lat_p, 'lon_p': lon_p}\n",
    "    \n",
    "    # 3. Distribution des labels (crimes)\n",
    "    if hasattr(train_df, '__label__') or '__label__' in train_df.columns:\n",
    "        from scipy.stats import chi2_contingency\n",
    "        \n",
    "        train_labels = train_df['__label__'] if '__label__' in train_df.columns else train_df.iloc[:, -1]\n",
    "        test_labels = test_df['__label__'] if '__label__' in test_df.columns else test_df.iloc[:, -1]\n",
    "        \n",
    "        train_counts = train_labels.value_counts().sort_index()\n",
    "        test_counts = test_labels.value_counts().sort_index()\n",
    "        \n",
    "        # Assurer mÃªme index\n",
    "        all_labels = train_counts.index.union(test_counts.index)\n",
    "        train_counts = train_counts.reindex(all_labels, fill_value=0)\n",
    "        test_counts = test_counts.reindex(all_labels, fill_value=0)\n",
    "        \n",
    "        contingency_table = pd.DataFrame({\n",
    "            'train': train_counts,\n",
    "            'test': test_counts\n",
    "        }).T\n",
    "        \n",
    "        chi2_stat, label_p, dof, expected = chi2_contingency(contingency_table)\n",
    "        print(f\"   Distribution labels - Chi2 test: p={label_p:.4f} {'âœ… OK' if label_p > 0.05 else 'âš ï¸ DRIFT'}\")\n",
    "        \n",
    "        results['label_drift_p'] = label_p\n",
    "    \n",
    "    # 4. Statistiques descriptives\n",
    "    print(f\"\\nğŸ“Š Comparaison statistiques Train vs Test:\")\n",
    "    for feature in features[:5]:  # Top 5 features\n",
    "        if feature in train_df.columns:\n",
    "            train_mean = train_df[feature].mean()\n",
    "            test_mean = test_df[feature].mean()\n",
    "            diff_pct = abs(train_mean - test_mean) / train_mean * 100 if train_mean != 0 else 0\n",
    "            \n",
    "            status = \"âœ… OK\" if diff_pct < 5 else \"âš ï¸ DIFF\" if diff_pct < 15 else \"ğŸ”´ DRIFT\"\n",
    "            print(f\"   {feature}: Train={train_mean:.3f} Test={test_mean:.3f} Diff={diff_pct:.1f}% {status}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExÃ©cuter l'analyse de drift\n",
    "drift_results = run_drift_analysis(train_dataset, test_dataset, \n",
    "                                  train_dataset if DEEPCHECKS_AVAILABLE else X_train, \n",
    "                                  test_dataset if DEEPCHECKS_AVAILABLE else X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le widget interactif Deepchecks (si disponible)\n",
    "if 'deepchecks_result' in drift_results:\n",
    "    print(\"ğŸ“Š Rapport interactif Deepchecks:\")\n",
    "    drift_results['deepchecks_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358cc40",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ Analyse de StabilitÃ© Temporelle (SpÃ©cifique Crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ac12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_stability_analysis(df):\n",
    "    \"\"\"Analyse spÃ©cifique Ã  la stabilitÃ© temporelle des crimes\"\"\"\n",
    "    print(\"\\nğŸ•’ ANALYSE DE STABILITÃ‰ TEMPORELLE\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Analyser les patterns temporels\n",
    "    if 'DATE OCC' in df.columns:\n",
    "        df_temp = df.copy()\n",
    "        df_temp['DATE OCC'] = pd.to_datetime(df_temp['DATE OCC'], errors='coerce')\n",
    "        df_temp = df_temp.dropna(subset=['DATE OCC'])\n",
    "        \n",
    "        # Grouper par mois\n",
    "        monthly_counts = df_temp.groupby(df_temp['DATE OCC'].dt.to_period('M')).size()\n",
    "        \n",
    "        # Coefficient de variation\n",
    "        cv = monthly_counts.std() / monthly_counts.mean()\n",
    "        print(f\"   Coefficient variation mensuel: {cv:.3f} {'âœ… Stable' if cv < 0.2 else 'âš ï¸ Variable'}\")\n",
    "        \n",
    "        # Distribution par jour de la semaine\n",
    "        if 'Day_of_week' in df.columns:\n",
    "            day_dist = df['Day_of_week'].value_counts().sort_index()\n",
    "            day_cv = day_dist.std() / day_dist.mean()\n",
    "            print(f\"   Variation jours semaine: {day_cv:.3f} {'âœ… Ã‰quilibrÃ©' if day_cv < 0.3 else 'âš ï¸ DÃ©sÃ©quilibrÃ©'}\")\n",
    "        \n",
    "        # Distribution par heure\n",
    "        if 'Hour' in df.columns:\n",
    "            hour_dist = df['Hour'].value_counts().sort_index()\n",
    "            # Les heures de nuit devraient avoir moins de crimes\n",
    "            night_crimes = hour_dist.loc[hour_dist.index.isin([0, 1, 2, 3, 4, 5])].sum()\n",
    "            total_crimes = hour_dist.sum()\n",
    "            night_pct = night_crimes / total_crimes * 100\n",
    "            \n",
    "            print(f\"   Crimes nocturnes (0h-5h): {night_pct:.1f}% {'âœ… Normal' if 5 <= night_pct <= 20 else 'âš ï¸ Atypique'}\")\n",
    "        \n",
    "        return {\n",
    "            'monthly_cv': cv,\n",
    "            'night_crimes_pct': night_pct if 'night_pct' in locals() else None\n",
    "        }\n",
    "    else:\n",
    "        print(\"   âš ï¸ Colonne DATE OCC manquante - analyse temporelle limitÃ©e\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExÃ©cuter l'analyse temporelle\n",
    "temporal_results = temporal_stability_analysis(df_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479ab7d0",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š RÃ©sumÃ© et Recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… NIVEAU 2 : DRIFT ET DISTRIBUTION - TERMINÃ‰\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š RÃ©sumÃ© Drift Analysis:\")\n",
    "print(f\"   Train samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features analysÃ©es: {len(features)}\")\n",
    "\n",
    "# RÃ©sumÃ© des tests statistiques\n",
    "drift_detected = False\n",
    "\n",
    "if 'hour_drift_p' in drift_results:\n",
    "    hour_ok = drift_results['hour_drift_p'] > 0.05\n",
    "    print(f\"   Drift heures: {'âœ… OK' if hour_ok else 'âš ï¸ DÃ‰TECTÃ‰'}\")\n",
    "    if not hour_ok:\n",
    "        drift_detected = True\n",
    "\n",
    "if 'geo_drift' in drift_results:\n",
    "    geo_ok = (drift_results['geo_drift']['lat_p'] > 0.05 and \n",
    "              drift_results['geo_drift']['lon_p'] > 0.05)\n",
    "    print(f\"   Drift gÃ©ographique: {'âœ… OK' if geo_ok else 'âš ï¸ DÃ‰TECTÃ‰'}\")\n",
    "    if not geo_ok:\n",
    "        drift_detected = True\n",
    "\n",
    "if 'label_drift_p' in drift_results:\n",
    "    label_ok = drift_results['label_drift_p'] > 0.05\n",
    "    print(f\"   Drift labels: {'âœ… OK' if label_ok else 'âš ï¸ DÃ‰TECTÃ‰'}\")\n",
    "    if not label_ok:\n",
    "        drift_detected = True\n",
    "\n",
    "# Ã‰valuation globale\n",
    "print(f\"\\nğŸ¯ Ã‰valuation Globale:\")\n",
    "if drift_detected:\n",
    "    print(f\"   Status: âš ï¸ DRIFT DÃ‰TECTÃ‰ - VÃ©rifications nÃ©cessaires\")\n",
    "    print(f\"   Action: Analyser les causes du drift et ajuster le split si nÃ©cessaire\")\n",
    "else:\n",
    "    print(f\"   Status: âœ… DISTRIBUTION STABLE - DonnÃ©es prÃªtes pour entraÃ®nement\")\n",
    "    print(f\"   Action: ProcÃ©der au NIVEAU 3 (Performance du modÃ¨le)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Prochaines Ã©tapes:\")\n",
    "if not drift_detected:\n",
    "    print(f\"   1. âœ… Split train/test validÃ©\")\n",
    "    print(f\"   2. ğŸš€ EntraÃ®ner les modÃ¨les\")\n",
    "    print(f\"   3. ğŸ”„ ExÃ©cuter: deepchecks_performance.ipynb\")\n",
    "else:\n",
    "    print(f\"   1. ğŸ”§ Investiguer les causes du drift dÃ©tectÃ©\")\n",
    "    print(f\"   2. âš–ï¸ ConsidÃ©rer un re-split stratifiÃ© diffÃ©rent\")\n",
    "    print(f\"   3. â™»ï¸ Re-exÃ©cuter ce notebook aprÃ¨s ajustements\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Rapports gÃ©nÃ©rÃ©s:\")\n",
    "for report_file in REPORTS_DIR.glob('*drift*.html'):\n",
    "    print(f\"   ğŸ“„ {report_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
